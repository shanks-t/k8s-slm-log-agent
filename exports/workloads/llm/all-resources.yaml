apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-07T11:17:40-06:00"
    creationTimestamp: "2025-12-07T17:17:41Z"
    generateName: llama-cpp-77c6884846-
    generation: 1
    labels:
      app: llama-cpp
      component: llm-server
      pod-template-hash: 77c6884846
    name: llama-cpp-77c6884846-2rj47
    namespace: llm
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: llama-cpp-77c6884846
      uid: 5eec2b7d-1a4e-48e7-88c5-31bcb1eadd75
    resourceVersion: "831131"
    uid: 7f2b8a15-cba7-4dab-af88-cd5dba9d27a4
  spec:
    containers:
    - args:
      - --model
      - /models/llama-3.2-3b-instruct-q4_k_m.gguf
      - --alias
      - llama-3.2-3b-instruct
      - --host
      - 0.0.0.0
      - --port
      - "8080"
      - --ctx-size
      - "4096"
      - --threads
      - "14"
      - --parallel
      - "2"
      - --n-gpu-layers
      - "0"
      - --metrics
      image: ghcr.io/ggml-org/llama.cpp:server
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: llama-server
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: "12"
          memory: 10Gi
        requests:
          cpu: "6"
          memory: 6Gi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /models
        name: models
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bsxhg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: node-2
    nodeSelector:
      hardware: heavy
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: heavy
      operator: Equal
      value: "true"
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: models
      persistentVolumeClaim:
        claimName: llama-models-pvc
    - name: kube-api-access-bsxhg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-07T17:17:43Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-07T17:17:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-07T17:18:14Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-07T17:18:14Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-07T17:17:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - allocatedResources:
        cpu: "6"
        memory: 6Gi
      containerID: containerd://b5728e3849eb1e930fac3fbd737d39feefe716be0b1124c8f8e3a628e5634e49
      image: ghcr.io/ggml-org/llama.cpp:server
      imageID: ghcr.io/ggml-org/llama.cpp@sha256:c6dc3bdf10137102b917ae5ccb9b44146ecd9a6cdd1b2280074fdb2e3a94170b
      lastState: {}
      name: llama-server
      ready: true
      resources:
        limits:
          cpu: "12"
          memory: 10Gi
        requests:
          cpu: "6"
          memory: 6Gi
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-07T17:17:43Z"
      user:
        linux:
          gid: 0
          supplementalGroups:
          - 0
          uid: 0
      volumeMounts:
      - mountPath: /models
        name: models
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bsxhg
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 10.0.0.103
    hostIPs:
    - ip: 10.0.0.103
    - ip: 2601:483:5781:d70::ddd1
    phase: Running
    podIP: 10.42.2.15
    podIPs:
    - ip: 10.42.2.15
    qosClass: Burstable
    startTime: "2025-12-07T17:17:42Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"llama-cpp","component":"llm-server"},"name":"llama-cpp","namespace":"llm"},"spec":{"ports":[{"name":"http","port":8080,"protocol":"TCP","targetPort":8080}],"selector":{"app":"llama-cpp"},"type":"ClusterIP"}}
    creationTimestamp: "2025-12-07T16:34:53Z"
    labels:
      app: llama-cpp
      component: llm-server
    name: llama-cpp
    namespace: llm
    resourceVersion: "698974"
    uid: a53adb5f-d59f-4502-bb50-e2e094556e07
  spec:
    clusterIP: 10.43.153.175
    clusterIPs:
    - 10.43.153.175
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: llama-cpp
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "10"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"llama-cpp","component":"llm-server"},"name":"llama-cpp","namespace":"llm"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"llama-cpp"}},"template":{"metadata":{"labels":{"app":"llama-cpp","component":"llm-server"}},"spec":{"containers":[{"args":["--model","/models/llama-3.2-3b-instruct-q4_k_m.gguf","--alias","llama-3.2-3b-instruct","--host","0.0.0.0","--port","8080","--ctx-size","4096","--threads","14","--parallel","2","--n-gpu-layers","0","--metrics"],"image":"ghcr.io/ggml-org/llama.cpp:server","imagePullPolicy":"Always","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/health","port":8080},"initialDelaySeconds":60,"periodSeconds":10,"timeoutSeconds":5},"name":"llama-server","ports":[{"containerPort":8080,"name":"http","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/health","port":8080},"initialDelaySeconds":30,"periodSeconds":5,"timeoutSeconds":3},"resources":{"limits":{"cpu":"12000m","memory":"10Gi"},"requests":{"cpu":"6000m","memory":"6Gi"}},"volumeMounts":[{"mountPath":"/models","name":"models","readOnly":true}]}],"nodeSelector":{"hardware":"heavy"},"tolerations":[{"effect":"NoSchedule","key":"heavy","operator":"Equal","value":"true"}],"volumes":[{"name":"models","persistentVolumeClaim":{"claimName":"llama-models-pvc"}}]}}}}
    creationTimestamp: "2025-12-07T16:34:53Z"
    generation: 10
    labels:
      app: llama-cpp
      component: llm-server
    name: llama-cpp
    namespace: llm
    resourceVersion: "831138"
    uid: ac51697c-7dd8-490b-a9ca-767560ace26b
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: llama-cpp
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-07T11:17:40-06:00"
        creationTimestamp: null
        labels:
          app: llama-cpp
          component: llm-server
      spec:
        containers:
        - args:
          - --model
          - /models/llama-3.2-3b-instruct-q4_k_m.gguf
          - --alias
          - llama-3.2-3b-instruct
          - --host
          - 0.0.0.0
          - --port
          - "8080"
          - --ctx-size
          - "4096"
          - --threads
          - "14"
          - --parallel
          - "2"
          - --n-gpu-layers
          - "0"
          - --metrics
          image: ghcr.io/ggml-org/llama.cpp:server
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: llama-server
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "12"
              memory: 10Gi
            requests:
              cpu: "6"
              memory: 6Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /models
            name: models
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          hardware: heavy
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-07T17:01:40Z"
      lastUpdateTime: "2025-12-07T17:18:14Z"
      message: ReplicaSet "llama-cpp-77c6884846" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-12-09T08:53:41Z"
      lastUpdateTime: "2025-12-09T08:53:41Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 10
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "9"
    creationTimestamp: "2025-12-07T17:17:37Z"
    generation: 3
    labels:
      app: llama-cpp
      component: llm-server
      pod-template-hash: 57c95d86b9
    name: llama-cpp-57c95d86b9
    namespace: llm
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: llama-cpp
      uid: ac51697c-7dd8-490b-a9ca-767560ace26b
    resourceVersion: "701797"
    uid: d89306fa-bb39-428e-9a02-b155aa3cd723
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: llama-cpp
        pod-template-hash: 57c95d86b9
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-07T11:15:43-06:00"
        creationTimestamp: null
        labels:
          app: llama-cpp
          component: llm-server
          pod-template-hash: 57c95d86b9
      spec:
        containers:
        - args:
          - --model
          - /models/llama-3.2-3b-instruct-q4_k_m.gguf
          - --alias
          - llama-3.2-3b-instruct
          - --host
          - 0.0.0.0
          - --port
          - "8080"
          - --ctx-size
          - "4096"
          - --threads
          - "14"
          - --parallel
          - "2"
          - --n-gpu-layers
          - "0"
          - --metrics
          image: ghcr.io/ggml-org/llama.cpp:server
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: llama-server
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "12"
              memory: 10Gi
            requests:
              cpu: "6"
              memory: 6Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /models
            name: models
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          hardware: heavy
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2025-12-07T17:09:44Z"
    generation: 3
    labels:
      app: llama-cpp
      component: llm-server
      pod-template-hash: 58969d45c5
    name: llama-cpp-58969d45c5
    namespace: llm
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: llama-cpp
      uid: ac51697c-7dd8-490b-a9ca-767560ace26b
    resourceVersion: "701526"
    uid: 93f365f9-6250-4fe9-a760-888a5188f7e7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: llama-cpp
        pod-template-hash: 58969d45c5
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-07T11:09:44-06:00"
        creationTimestamp: null
        labels:
          app: llama-cpp
          component: llm-server
          pod-template-hash: 58969d45c5
      spec:
        containers:
        - args:
          - --model
          - /models/llama-3.2-3b-instruct-q4_k_m.gguf
          - --alias
          - llama-3.2-3b-instruct
          - --host
          - 0.0.0.0
          - --port
          - "8080"
          - --ctx-size
          - "4096"
          - --threads
          - "14"
          - --parallel
          - "2"
          - --n-gpu-layers
          - "0"
          - --metrics
          - --api
          image: ghcr.io/ggerganov/llama.cpp:server
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: llama-server
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "12"
              memory: 10Gi
            requests:
              cpu: "6"
              memory: 6Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /models
            name: models
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          hardware: heavy
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "8"
    creationTimestamp: "2025-12-07T17:15:43Z"
    generation: 3
    labels:
      app: llama-cpp
      component: llm-server
      pod-template-hash: 59d4f8cd4c
    name: llama-cpp-59d4f8cd4c
    namespace: llm
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: llama-cpp
      uid: ac51697c-7dd8-490b-a9ca-767560ace26b
    resourceVersion: "701755"
    uid: fbb56e08-1889-44e1-90b2-744082d353ac
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: llama-cpp
        pod-template-hash: 59d4f8cd4c
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-07T11:15:43-06:00"
        creationTimestamp: null
        labels:
          app: llama-cpp
          component: llm-server
          pod-template-hash: 59d4f8cd4c
      spec:
        containers:
        - args:
          - --model
          - /models/llama-3.2-3b-instruct-q4_k_m.gguf
          - --alias
          - llama-3.2-3b-instruct
          - --host
          - 0.0.0.0
          - --port
          - "8080"
          - --ctx-size
          - "4096"
          - --threads
          - "14"
          - --parallel
          - "2"
          - --n-gpu-layers
          - "0"
          - --metrics
          - --api
          image: ghcr.io/ggml-org/llama.cpp:server
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: llama-server
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "12"
              memory: 10Gi
            requests:
              cpu: "6"
              memory: 6Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /models
            name: models
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          hardware: heavy
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2025-12-07T17:15:39Z"
    generation: 3
    labels:
      app: llama-cpp
      component: llm-server
      pod-template-hash: 5b8c8f8545
    name: llama-cpp-5b8c8f8545
    namespace: llm
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: llama-cpp
      uid: ac51697c-7dd8-490b-a9ca-767560ace26b
    resourceVersion: "701569"
    uid: 2b59c564-83c3-422a-b807-4fff306dd54c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: llama-cpp
        pod-template-hash: 5b8c8f8545
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-07T11:09:44-06:00"
        creationTimestamp: null
        labels:
          app: llama-cpp
          component: llm-server
          pod-template-hash: 5b8c8f8545
      spec:
        containers:
        - args:
          - --model
          - /models/llama-3.2-3b-instruct-q4_k_m.gguf
          - --alias
          - llama-3.2-3b-instruct
          - --host
          - 0.0.0.0
          - --port
          - "8080"
          - --ctx-size
          - "4096"
          - --threads
          - "14"
          - --parallel
          - "2"
          - --n-gpu-layers
          - "0"
          - --metrics
          - --api
          image: ghcr.io/ggml-org/llama.cpp:server
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: llama-server
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "12"
              memory: 10Gi
            requests:
              cpu: "6"
              memory: 6Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /models
            name: models
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          hardware: heavy
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-12-07T16:51:25Z"
    generation: 2
    labels:
      app: llama-cpp
      component: llm-server
      pod-template-hash: 5c46b6c6cc
    name: llama-cpp-5c46b6c6cc
    namespace: llm
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: llama-cpp
      uid: ac51697c-7dd8-490b-a9ca-767560ace26b
    resourceVersion: "700589"
    uid: 78662266-1d9b-4e48-8ea9-289014189c72
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: llama-cpp
        pod-template-hash: 5c46b6c6cc
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: llama-cpp
          component: llm-server
          pod-template-hash: 5c46b6c6cc
      spec:
        containers:
        - args:
          - --model
          - /models/llama-3.2-3b-instruct-q4_k_m.gguf
          - --alias
          - llama-3.2-3b-instruct
          - --host
          - 0.0.0.0
          - --port
          - "8080"
          - --ctx-size
          - "4096"
          - --threads
          - "14"
          - --parallel
          - "2"
          - --n-gpu-layers
          - "0"
          - --metrics
          image: ghcr.io/ggerganov/llama.cpp:server
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: llama-server
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "14"
              memory: 10Gi
            requests:
              cpu: "10"
              memory: 6Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /models
            name: models
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          hardware: heavy
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2025-12-07T17:09:25Z"
    generation: 3
    labels:
      app: llama-cpp
      component: llm-server
      pod-template-hash: 675c458dd5
    name: llama-cpp-675c458dd5
    namespace: llm
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: llama-cpp
      uid: ac51697c-7dd8-490b-a9ca-767560ace26b
    resourceVersion: "701110"
    uid: 2857a678-46c0-4ebf-943c-4ed6448b40ef
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: llama-cpp
        pod-template-hash: 675c458dd5
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: llama-cpp
          component: llm-server
          pod-template-hash: 675c458dd5
      spec:
        containers:
        - args:
          - --model
          - /models/llama-3.2-3b-instruct-q4_k_m.gguf
          - --alias
          - llama-3.2-3b-instruct
          - --host
          - 0.0.0.0
          - --port
          - "8080"
          - --ctx-size
          - "4096"
          - --threads
          - "14"
          - --parallel
          - "2"
          - --n-gpu-layers
          - "0"
          - --metrics
          - --api
          image: ghcr.io/ggerganov/llama.cpp:server
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: llama-server
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "12"
              memory: 10Gi
            requests:
              cpu: "6"
              memory: 6Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /models
            name: models
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          hardware: heavy
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "10"
    creationTimestamp: "2025-12-07T17:17:40Z"
    generation: 2
    labels:
      app: llama-cpp
      component: llm-server
      pod-template-hash: 77c6884846
    name: llama-cpp-77c6884846
    namespace: llm
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: llama-cpp
      uid: ac51697c-7dd8-490b-a9ca-767560ace26b
    resourceVersion: "831135"
    uid: 5eec2b7d-1a4e-48e7-88c5-31bcb1eadd75
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: llama-cpp
        pod-template-hash: 77c6884846
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-07T11:17:40-06:00"
        creationTimestamp: null
        labels:
          app: llama-cpp
          component: llm-server
          pod-template-hash: 77c6884846
      spec:
        containers:
        - args:
          - --model
          - /models/llama-3.2-3b-instruct-q4_k_m.gguf
          - --alias
          - llama-3.2-3b-instruct
          - --host
          - 0.0.0.0
          - --port
          - "8080"
          - --ctx-size
          - "4096"
          - --threads
          - "14"
          - --parallel
          - "2"
          - --n-gpu-layers
          - "0"
          - --metrics
          image: ghcr.io/ggml-org/llama.cpp:server
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: llama-server
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "12"
              memory: 10Gi
            requests:
              cpu: "6"
              memory: 6Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /models
            name: models
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          hardware: heavy
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-12-07T17:01:40Z"
    generation: 3
    labels:
      app: llama-cpp
      component: llm-server
      pod-template-hash: 86449b7d55
    name: llama-cpp-86449b7d55
    namespace: llm
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: llama-cpp
      uid: ac51697c-7dd8-490b-a9ca-767560ace26b
    resourceVersion: "700680"
    uid: f12d6b15-4964-4d10-bb74-888555b740b2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: llama-cpp
        pod-template-hash: 86449b7d55
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: llama-cpp
          component: llm-server
          pod-template-hash: 86449b7d55
      spec:
        containers:
        - args:
          - --model
          - /models/llama-3.2-3b-instruct-q4_k_m.gguf
          - --host
          - 0.0.0.0
          - --port
          - "8080"
          - --ctx-size
          - "4096"
          - --threads
          - "14"
          - --parallel
          - "2"
          - --n-gpu-layers
          - "0"
          - --metrics
          - --api
          image: ghcr.io/ggerganov/llama.cpp:server
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: llama-server
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "14"
              memory: 10Gi
            requests:
              cpu: "10"
              memory: 6Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /models
            name: models
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          hardware: heavy
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-12-07T16:34:53Z"
    generation: 2
    labels:
      app: llama-cpp
      component: llm-server
      pod-template-hash: 864d66db97
    name: llama-cpp-864d66db97
    namespace: llm
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: llama-cpp
      uid: ac51697c-7dd8-490b-a9ca-767560ace26b
    resourceVersion: "701862"
    uid: 545c6a62-6676-4bc1-82d7-9d53775894a0
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: llama-cpp
        pod-template-hash: 864d66db97
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: llama-cpp
          component: llm-server
          pod-template-hash: 864d66db97
      spec:
        containers:
        - args:
          - --model
          - /models/llama-3.2-3b-instruct-q4_k_m.gguf
          - --host
          - 0.0.0.0
          - --port
          - "8080"
          - --ctx-size
          - "4096"
          - --threads
          - "14"
          - --parallel
          - "2"
          - --n-gpu-layers
          - "0"
          - --metrics
          image: ghcr.io/ggerganov/llama.cpp:server
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: llama-server
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "14"
              memory: 10Gi
            requests:
              cpu: "10"
              memory: 6Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /models
            name: models
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          hardware: heavy
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2025-12-07T17:02:54Z"
    generation: 3
    labels:
      app: llama-cpp
      component: llm-server
      pod-template-hash: fccbf48ff
    name: llama-cpp-fccbf48ff
    namespace: llm
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: llama-cpp
      uid: ac51697c-7dd8-490b-a9ca-767560ace26b
    resourceVersion: "701057"
    uid: 43b90ba5-4221-47c2-9b45-528a4a2c1aac
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: llama-cpp
        pod-template-hash: fccbf48ff
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: llama-cpp
          component: llm-server
          pod-template-hash: fccbf48ff
      spec:
        containers:
        - args:
          - --model
          - /models/llama-3.2-3b-instruct-q4_k_m.gguf
          - --alias
          - llama-3.2-3b-instruct
          - --host
          - 0.0.0.0
          - --port
          - "8080"
          - --ctx-size
          - "4096"
          - --threads
          - "14"
          - --parallel
          - "2"
          - --n-gpu-layers
          - "0"
          - --metrics
          - --api
          image: ghcr.io/ggerganov/llama.cpp:server
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: llama-server
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "14"
              memory: 10Gi
            requests:
              cpu: "10"
              memory: 6Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /models
            name: models
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          hardware: heavy
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: v1
  data:
    ca.crt: |
      -----BEGIN CERTIFICATE-----
      MIIBdjCCAR2gAwIBAgIBADAKBggqhkjOPQQDAjAjMSEwHwYDVQQDDBhrM3Mtc2Vy
      dmVyLWNhQDE3NjQzNDg5NzUwHhcNMjUxMTI4MTY1NjE1WhcNMzUxMTI2MTY1NjE1
      WjAjMSEwHwYDVQQDDBhrM3Mtc2VydmVyLWNhQDE3NjQzNDg5NzUwWTATBgcqhkjO
      PQIBBggqhkjOPQMBBwNCAAQ25s4put72uOZJTE+6CkVsi3PgJDgb6AJGaf7PuWrR
      8qzDoW8b5nKBUgLX3zE6y2d5761/3EV7h8o2WuN/Aqv/o0IwQDAOBgNVHQ8BAf8E
      BAMCAqQwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUZ8gy9U93ulmOr1qFrsJa
      smmR/wowCgYIKoZIzj0EAwIDRwAwRAIgXD3uTRtnLW9Bbn1T6rzvlkPRZPPYLb0+
      wNMG7hhi86UCIFef9l2XoftW2ozQNEysH07oqFNNvXykHT0sv9g6Ajs8
      -----END CERTIFICATE-----
  kind: ConfigMap
  metadata:
    annotations:
      kubernetes.io/description: Contains a CA bundle that can be used to verify the
        kube-apiserver when using internal endpoints such as the internal service
        IP or kubernetes.default.svc. No other usage is guaranteed across distributions
        of Kubernetes clusters.
    creationTimestamp: "2025-12-07T16:34:20Z"
    name: kube-root-ca.crt
    namespace: llm
    resourceVersion: "698924"
    uid: c3824a18-7457-4c09-adb9-c0c0c9c657a2
- apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"llama-models-pvc","namespace":"llm"},"spec":{"accessModes":["ReadOnlyMany"],"resources":{"requests":{"storage":"20Gi"}},"selector":{"matchLabels":{"app":"llama-cpp"}},"storageClassName":"local-storage"}}
      pv.kubernetes.io/bind-completed: "yes"
      pv.kubernetes.io/bound-by-controller: "yes"
    creationTimestamp: "2025-12-07T16:34:53Z"
    finalizers:
    - kubernetes.io/pvc-protection
    name: llama-models-pvc
    namespace: llm
    resourceVersion: "698957"
    uid: 1fce8c54-28e0-470c-8c59-450ba0bdeb41
  spec:
    accessModes:
    - ReadOnlyMany
    resources:
      requests:
        storage: 20Gi
    selector:
      matchLabels:
        app: llama-cpp
    storageClassName: local-storage
    volumeMode: Filesystem
    volumeName: llama-models-pv
  status:
    accessModes:
    - ReadOnlyMany
    capacity:
      storage: 20Gi
    phase: Bound
kind: List
metadata:
  resourceVersion: ""
