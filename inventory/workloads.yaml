apiVersion: v1
items:
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"test-app","namespace":"default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"test-app"}},"template":{"metadata":{"labels":{"app":"test-app"}},"spec":{"containers":[{"args":["-text=Hello from homelab gateway!","-listen=:8080"],"image":"hashicorp/http-echo:latest","name":"echo","ports":[{"containerPort":8080}]}]}}}}
    creationTimestamp: "2025-11-28T18:06:19Z"
    generation: 2
    name: test-app
    namespace: default
    resourceVersion: "627237"
    uid: bc3ac7e0-e641-42ee-b24d-61b9df6c22c5
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: test-app
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: test-app
      spec:
        containers:
        - args:
          - -text=Hello from homelab gateway!
          - -listen=:8080
          image: hashicorp/http-echo:latest
          imagePullPolicy: Always
          name: echo
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-28T18:06:19Z"
      lastUpdateTime: "2025-11-28T18:48:28Z"
      message: ReplicaSet "test-app-5fc5d4895d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-12-06T18:40:14Z"
      lastUpdateTime: "2025-12-06T18:40:14Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-28T18:42:15Z"
    generation: 1
    labels:
      app.kubernetes.io/component: proxy
      app.kubernetes.io/managed-by: envoy-gateway
      app.kubernetes.io/name: envoy
      gateway.envoyproxy.io/owning-gateway-name: homelab-gateway
      gateway.envoyproxy.io/owning-gateway-namespace: envoy-gateway-system
    name: envoy-envoy-gateway-system-homelab-gateway-00f55f79
    namespace: envoy-gateway-system
    resourceVersion: "2216552"
    uid: 209e189b-70c5-4d64-b5de-59167503b273
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: proxy
        app.kubernetes.io/managed-by: envoy-gateway
        app.kubernetes.io/name: envoy
        gateway.envoyproxy.io/owning-gateway-name: homelab-gateway
        gateway.envoyproxy.io/owning-gateway-namespace: envoy-gateway-system
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /stats/prometheus
          prometheus.io/port: "19001"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: proxy
          app.kubernetes.io/managed-by: envoy-gateway
          app.kubernetes.io/name: envoy
          gateway.envoyproxy.io/owning-gateway-name: homelab-gateway
          gateway.envoyproxy.io/owning-gateway-namespace: envoy-gateway-system
      spec:
        automountServiceAccountToken: false
        containers:
        - args:
          - --service-cluster envoy-gateway-system/homelab-gateway
          - --service-node $(ENVOY_POD_NAME)
          - |
            --config-yaml admin:
              access_log:
              - name: envoy.access_loggers.file
                typed_config:
                  "@type": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog
                  path: /dev/null
              address:
                socket_address:
                  address: 127.0.0.1
                  port_value: 19000
            cluster_manager:
              local_cluster_name: local_cluster
            node:
              locality:
                zone: $(ENVOY_SERVICE_ZONE)
            layered_runtime:
              layers:
              - name: global_config
                static_layer:
                  envoy.restart_features.use_eds_cache_for_ads: true
                  re2.max_program_size.error_level: 4294967295
                  re2.max_program_size.warn_level: 1000
            dynamic_resources:
              ads_config:
                api_type: DELTA_GRPC
                transport_api_version: V3
                grpc_services:
                - envoy_grpc:
                    cluster_name: xds_cluster
                set_node_on_first_message_only: true
              lds_config:
                ads: {}
                resource_api_version: V3
              cds_config:
                ads: {}
                resource_api_version: V3
            static_resources:
              listeners:
              - name: envoy-gateway-proxy-stats-0.0.0.0-19001
                address:
                  socket_address:
                    address: '0.0.0.0'
                    port_value: 19001
                    protocol: TCP
                bypass_overload_manager: true
                filter_chains:
                - filters:
                  - name: envoy.filters.network.http_connection_manager
                    typed_config:
                      "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
                      stat_prefix: eg-stats-http
                      normalize_path: true
                      route_config:
                        name: local_route
                        virtual_hosts:
                        - name: prometheus_stats
                          domains:
                          - "*"
                          routes:
                          - match:
                              path: /stats/prometheus
                              headers:
                              - name: ":method"
                                string_match:
                                  exact: GET
                            route:
                              cluster: prometheus_stats
                      http_filters:
                      - name: envoy.filters.http.router
                        typed_config:
                          "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
              clusters:
              - name: prometheus_stats
                connect_timeout: 0.250s
                type: STATIC
                lb_policy: ROUND_ROBIN
                load_assignment:
                  cluster_name: prometheus_stats
                  endpoints:
                  - lb_endpoints:
                    - endpoint:
                        address:
                          socket_address:
                            address: 127.0.0.1
                            port_value: 19000
              - connect_timeout: 10s
                lb_policy: ROUND_ROBIN
                load_assignment:
                  cluster_name: local_cluster
                  endpoints:
                  - lb_endpoints:
                    - endpoint:
                        address:
                          socket_address:
                            address: 127.0.0.1
                            port_value: 10080
                      load_balancing_weight: 1
                    load_balancing_weight: 1
                    locality:
                      zone: $(ENVOY_SERVICE_ZONE)
                name: local_cluster
                type: STATIC
              - connect_timeout: 10s
                load_assignment:
                  cluster_name: xds_cluster
                  endpoints:
                  - load_balancing_weight: 1
                    lb_endpoints:
                    - load_balancing_weight: 1
                      endpoint:
                        address:
                          socket_address:
                            address: envoy-gateway.envoy-gateway-system.svc.cluster.local
                            port_value: 18000
                typed_extension_protocol_options:
                  envoy.extensions.upstreams.http.v3.HttpProtocolOptions:
                    "@type": "type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions"
                    explicit_http_config:
                      http2_protocol_options:
                        connection_keepalive:
                          interval: 30s
                          timeout: 5s
                name: xds_cluster
                type: STRICT_DNS
                transport_socket:
                  name: envoy.transport_sockets.tls
                  typed_config:
                    "@type": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext
                    common_tls_context:
                      tls_params:
                        tls_maximum_protocol_version: TLSv1_3
                      tls_certificate_sds_secret_configs:
                      - name: xds_certificate
                        sds_config:
                          path_config_source:
                            path: /sds/xds-certificate.json
                          resource_api_version: V3
                      validation_context_sds_secret_config:
                        name: xds_trusted_ca
                        sds_config:
                          path_config_source:
                            path: /sds/xds-trusted-ca.json
                          resource_api_version: V3
              - name: wasm_cluster
                type: STRICT_DNS
                connect_timeout: 10s
                load_assignment:
                  cluster_name: wasm_cluster
                  endpoints:
                  - load_balancing_weight: 1
                    lb_endpoints:
                    - load_balancing_weight: 1
                      endpoint:
                        address:
                          socket_address:
                            address: envoy-gateway
                            port_value: 18002
                typed_extension_protocol_options:
                  envoy.extensions.upstreams.http.v3.HttpProtocolOptions:
                    "@type": "type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions"
                    explicit_http_config:
                      http2_protocol_options: {}
                transport_socket:
                  name: envoy.transport_sockets.tls
                  typed_config:
                    "@type": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext
                    common_tls_context:
                      tls_params:
                        tls_maximum_protocol_version: TLSv1_3
                      tls_certificate_sds_secret_configs:
                      - name: xds_certificate
                        sds_config:
                          path_config_source:
                            path: /sds/xds-certificate.json
                          resource_api_version: V3
                      validation_context_sds_secret_config:
                        name: xds_trusted_ca
                        sds_config:
                          path_config_source:
                            path: /sds/xds-trusted-ca.json
                          resource_api_version: V3
            overload_manager:
              refresh_interval: 0.25s
              resource_monitors:
              - name: "envoy.resource_monitors.global_downstream_max_connections"
                typed_config:
                  "@type": type.googleapis.com/envoy.extensions.resource_monitors.downstream_connections.v3.DownstreamConnectionsConfig
                  max_active_downstream_connections: 50000
          - --log-level warn
          - --cpuset-threads
          - --drain-strategy immediate
          - --drain-time-s 60
          command:
          - envoy
          env:
          - name: ENVOY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: ENVOY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: ENVOY_SERVICE_ZONE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.annotations['topology.kubernetes.io/zone']
          image: docker.io/envoyproxy/envoy:distroless-v1.34.10
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              httpGet:
                path: /shutdown/ready
                port: 19002
                scheme: HTTP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 19003
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: envoy
          ports:
          - containerPort: 19001
            name: metrics
            protocol: TCP
          - containerPort: 19003
            name: readiness
            protocol: TCP
          readinessProbe:
            failureThreshold: 1
            httpGet:
              path: /ready
              port: 19003
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
            seccompProfile:
              type: RuntimeDefault
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /ready
              port: 19003
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certs
            readOnly: true
          - mountPath: /sds
            name: sds
        - args:
          - envoy
          - shutdown-manager
          command:
          - envoy-gateway
          env:
          - name: ENVOY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: ENVOY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: ENVOY_SERVICE_ZONE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.annotations['topology.kubernetes.io/zone']
          image: envoyproxy/gateway:v1.4.6
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - envoy-gateway
                - envoy
                - shutdown
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 19002
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: shutdown-manager
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 19002
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 10m
              memory: 32Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
            seccompProfile:
              type: RuntimeDefault
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /healthz
              port: 19002
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: envoy-envoy-gateway-system-homelab-gateway-00f55f79
        serviceAccountName: envoy-envoy-gateway-system-homelab-gateway-00f55f79
        terminationGracePeriodSeconds: 360
        volumes:
        - name: certs
          secret:
            defaultMode: 420
            secretName: envoy
        - configMap:
            defaultMode: 420
            items:
            - key: xds-trusted-ca.json
              path: xds-trusted-ca.json
            - key: xds-certificate.json
              path: xds-certificate.json
            name: envoy-envoy-gateway-system-homelab-gateway-00f55f79
            optional: false
          name: sds
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-28T18:42:15Z"
      lastUpdateTime: "2025-11-28T18:42:26Z"
      message: ReplicaSet "envoy-envoy-gateway-system-homelab-gateway-00f55f79-75646f77bd"
        has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-12-27T01:32:35Z"
      lastUpdateTime: "2025-12-27T01:32:35Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app.kubernetes.io/instance":"eg","app.kubernetes.io/managed-by":"Helm","app.kubernetes.io/name":"gateway-helm","app.kubernetes.io/version":"latest","control-plane":"envoy-gateway","helm.sh/chart":"gateway-helm-v0.0.0-latest"},"name":"envoy-gateway","namespace":"envoy-gateway-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"app.kubernetes.io/instance":"eg","app.kubernetes.io/name":"gateway-helm","control-plane":"envoy-gateway"}},"template":{"metadata":{"annotations":{"prometheus.io/port":"19001","prometheus.io/scrape":"true"},"labels":{"app.kubernetes.io/instance":"eg","app.kubernetes.io/name":"gateway-helm","control-plane":"envoy-gateway"}},"spec":{"containers":[{"args":["server","--config-path=/config/envoy-gateway.yaml"],"env":[{"name":"ENVOY_GATEWAY_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}},{"name":"KUBERNETES_CLUSTER_DOMAIN","value":"cluster.local"}],"image":"envoyproxy/gateway:v1.4.6","imagePullPolicy":"IfNotPresent","livenessProbe":{"httpGet":{"path":"/healthz","port":8081},"initialDelaySeconds":15,"periodSeconds":20},"name":"envoy-gateway","ports":[{"containerPort":18000,"name":"grpc"},{"containerPort":18001,"name":"ratelimit"},{"containerPort":18002,"name":"wasm"},{"containerPort":19001,"name":"metrics"},{"containerPort":9443,"name":"webhook"}],"readinessProbe":{"httpGet":{"path":"/readyz","port":8081},"initialDelaySeconds":5,"periodSeconds":10},"resources":{"limits":{"memory":"1024Mi"},"requests":{"cpu":"100m","memory":"256Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"privileged":false,"runAsGroup":65532,"runAsNonRoot":true,"runAsUser":65532,"seccompProfile":{"type":"RuntimeDefault"}},"volumeMounts":[{"mountPath":"/config","name":"envoy-gateway-config","readOnly":true},{"mountPath":"/certs","name":"certs","readOnly":true}]}],"imagePullSecrets":[],"serviceAccountName":"envoy-gateway","terminationGracePeriodSeconds":10,"volumes":[{"configMap":{"defaultMode":420,"name":"envoy-gateway-config"},"name":"envoy-gateway-config"},{"name":"certs","secret":{"secretName":"envoy-gateway"}}]}}}}
      meta.helm.sh/release-name: eg
      meta.helm.sh/release-namespace: envoy-gateway-system
    creationTimestamp: "2025-11-28T17:40:07Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: eg
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: gateway-helm
      app.kubernetes.io/version: latest
      control-plane: envoy-gateway
      helm.sh/chart: gateway-helm-v0.0.0-latest
    name: envoy-gateway
    namespace: envoy-gateway-system
    resourceVersion: "2215228"
    uid: 4f26554c-df87-4ca3-944b-66abef07e05f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: eg
        app.kubernetes.io/name: gateway-helm
        control-plane: envoy-gateway
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/port: "19001"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: eg
          app.kubernetes.io/name: gateway-helm
          control-plane: envoy-gateway
      spec:
        containers:
        - args:
          - server
          - --config-path=/config/envoy-gateway.yaml
          env:
          - name: ENVOY_GATEWAY_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: KUBERNETES_CLUSTER_DOMAIN
            value: cluster.local
          image: envoyproxy/gateway:v1.4.6
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          name: envoy-gateway
          ports:
          - containerPort: 18000
            name: grpc
            protocol: TCP
          - containerPort: 18001
            name: ratelimit
            protocol: TCP
          - containerPort: 18002
            name: wasm
            protocol: TCP
          - containerPort: 19001
            name: metrics
            protocol: TCP
          - containerPort: 9443
            name: webhook
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: envoy-gateway-config
            readOnly: true
          - mountPath: /certs
            name: certs
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: envoy-gateway
        serviceAccountName: envoy-gateway
        terminationGracePeriodSeconds: 10
        volumes:
        - configMap:
            defaultMode: 420
            name: envoy-gateway-config
          name: envoy-gateway-config
        - name: certs
          secret:
            defaultMode: 420
            secretName: envoy-gateway
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-28T17:40:07Z"
      lastUpdateTime: "2025-11-28T18:17:45Z"
      message: ReplicaSet "envoy-gateway-775d8bf958" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-12-27T01:12:07Z"
      lastUpdateTime: "2025-12-27T01:12:07Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xV3XLbOA99lW9wLcVW3DauZr6Lbpzddtp6PXXSm06mQ1OQxTVFcEnIiTbjd9+BZDt2m592p1e2SAA8ODgA7kB58xlDNOQgB+V9HKwzSGBlXAE5TNBbamt0DAnUyKpQrCC/A+UcsWJDLsonLf5CzRH5JBg60YrZ4omhgZEgkDx6TzcOQ7pcryCH1Sge3Kyz5H/vjSv+/6YoyD0bwqkaIQdNAQsXf8g8eqXFZ9UsMI1tZKxhk4BVC7RdUqtxTJX3O5M+rvwNDhmjRNs+e04BJ9P5E89WKlaQw0Lj6Xh0+no8zrKzFyM1HI1fqcXLbFielq/OsDx7cfpiqF+eCZDvUnoCdPSoBXLAtZFavjWRKbQfTG0Y8mECES1qpiBGtWJdfXgqzY2E5KAYl20Xlqw1bnnlC8XYh7i9cmqtjFULi5BnmwS49YLs05GtnGPt7c7vQEJPEr05SEqTY2Uchgj5lztQYSl/INXkSkhggKwHW5YGUonSWITrBEytloIoKKcrDIPahCBm6dZ495tnJ9noRFTfecwaa2dkjW4hh3fllHgWMPYtYM0aHcY4C7ToEiqVsU3AyypgrMgWkI8SqJj9H8hy7xVL3QcVKssVJOApMOTj4ViKoivsavz28nImVBln2Cg7QavaOWpyRYT81TABj8FQsT/KxLnRGmM8eDlLgE2N1PC94UM6Egg9lXtmZx2ql6O99dYyEJMmCzlcTQThMy4pa3/sdnn+oNvr7MCxRg5GxwccrxMIqArznygXz/ae8Wyc/Sjj3xN++hN8B4zUBI2dtK10YOylX1MQSWVnw48GOsO/G4z9rfaNXA2HdTdot6a9pbQC6iYYbs/JMd52aSpr6WYWzNpYXOJF1Mp28xjyUtmICWjl1cJYw6aHoopC2mZ6cfn1t3fTydf5xafP784vpFOKQF7ulLVwvelJ/9PZ9hMR/24sbgdNzqHBTQJrsk2NH6lxWx3V8ne25f2gHeFAfa40y7T3hPsXdjEfjzHQTWSqD0J13+kzEa9FPIWL+06eYKkaK03sqMD5wTw8HukUIQdrXHMrNfLBUEe8VTFOewA9G6m2TWQMqQ6GjVYWpExhbTS+0VqSmX7beEwWw25pfrmDFQqw861/t+hil0IC5MVS8MHFrRGRCEdYlqgZcpjSXFdYNFYy78NIVmkgiyfH+UjnBbKpt8rhL41cK8n/4ZDXkq0nS8t27qU05+Rko5idZLrpP//prVSr2/kKb/rm2z7wvkN5jK2iyJ1eErip0F25qNjE0vTrCiY0Jd4nKgT8cjy7r29I+4fcY6B2eN649ka1HYe9uvfDujTLj8oLHMNYH4lot/eS3fzbn0h2vdGUCnxLUp+91f2RPPfNqtg80r7bgX6P5tgv3XcseRG7svvJ8VQLb643m83m3wAAAP//gh7fNZIKAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: coredns
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-11-28T16:56:24Z"
    generation: 2
    labels:
      k8s-app: kube-dns
      kubernetes.io/name: CoreDNS
      objectset.rio.cattle.io/hash: bce283298811743a0386ab510f2f67ef74240c57
    name: coredns
    namespace: kube-system
    resourceVersion: "2215175"
    uid: 7085c9c6-376e-4fbf-988c-5441b4ccb3f6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-26T18:06:53-06:00"
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: rancher/mirrored-coredns-coredns:1.13.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
          - mountPath: /etc/coredns/custom
            name: custom-config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              k8s-app: kube-dns
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        - labelSelector:
            matchLabels:
              k8s-app: kube-dns
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            - key: NodeHosts
              path: NodeHosts
            name: coredns
          name: config-volume
        - configMap:
            defaultMode: 420
            name: coredns-custom
            optional: true
          name: custom-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-28T16:56:26Z"
      lastUpdateTime: "2025-11-28T16:56:26Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-28T16:56:26Z"
      lastUpdateTime: "2025-12-27T00:06:55Z"
      message: ReplicaSet "coredns-69bcdf6c4d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xU4U/7NhD9V6b7nKTtr1BVkfahAqZNg1KB2BdUTVfn0po6tmVfM6Iq//t0TQpFo7BJ+5Q4vnt5796z94Be/0EhamchB/Q+DuoRJLDVtoAcrskb11RkGRKoiLFARsj3gNY6RtbORlm61QspjsRZ0C5TyGwo026gBQSSs/vuL0shXddbyGE7jic79Sj56Xdti59nReHstxAWK4IcjFNo0sgu4Jr+VVP0qKRzu1tRGpvIVEGbgMEVmS+lbTBuIIfRdFyOL9XksixXajycXEyG4/JiXI4up8NiqiZT/FHgqrgQ0A8kPfIm9cHVWoZPAbr9M3yiJyVsAnX1v2oR2dzqSjPkwwQiGVLsghRVyGpz+6YAvT//11bAOSDTujn8wBmj7frJF8jUgb0+WaxRG1wZgnzUJsCNF44PH2rlO1XeHPtO0mL+A5deqHKWUVsKEfJnWVYVSiSfz48vMgbJaZoqZ0u9hgQGxGrQrfpH9hKdhWUCZOsDcm/K4v76z/ns7uZxMbu6gQRqNDv6JbhKyJSaTPFA5dv7AlnMP2rM3p1r23aZgK4kfzkEtGpDYfA557weZsNs/AP6hsXOmIUzWjWQw2/l3PEiUOwO33fZqZ3ZVXTndpa7iVXy2vM8HcM7Vvch7TqhXQpx6wp6PImSxDBYYoqHYxOFgra7V3HbB+2C5ubKYIzzDrNLbCowqQqatUIj1lCotaKZUsJq/pWWtK9NsSuGBNgZCsfL5nkPW5IBXfXwhwsi3lvTyIH3Uinc4eZVR47QJnugsiTFkMPcPaoNFTsjl0MHc6AanKHso1YJYHAm9QYt/a/IFUY+ePYJ5PLo5DH2YtEdevHinxHoc96et7Rt278DAAD//8uQvPTkBQAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: local-storage
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-11-28T16:56:24Z"
    generation: 1
    labels:
      objectset.rio.cattle.io/hash: 183f35c65ffbc3064603f43f1580d8c68a2dabd4
    name: local-path-provisioner
    namespace: kube-system
    resourceVersion: "627228"
    uid: c95b2b5e-71b5-405f-b18a-e058a95c3855
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        app: local-path-provisioner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
      spec:
        containers:
        - command:
          - local-path-provisioner
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.32
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-28T16:56:26Z"
      lastUpdateTime: "2025-11-28T16:56:26Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-28T16:56:26Z"
      lastUpdateTime: "2025-11-28T16:56:31Z"
      message: ReplicaSet "local-path-provisioner-869c44bfbd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xV4W8auRP9V36az7vJLoEkXYkPiPAr1aUpKvROpypCxjuAD6/t88zScBH/+2l2k5RcStOr7gta7PHzm/fGM/eggvkVIxnvoAAVAp1uc0hgY1wJBVxhsH5XoWNIoEJWpWIFxT0o5zwrNt6R/PWLP1AzIZ9E40+0YrZ4YvypERBIju77Lw5jutpuoIDNGR3sbPPkf78YV/YHZendqxBOVQiFUIxGU0oYtxjT8pD+6wAUlBaUTb3AlHbEWME+AasWaJs0N5eUqhBeXPQd9LWiNRSAeYadbg/zM+yeq4tOT78py7LsLt/gonP2Zpl3Ly4uustS7vtmLtCuH6FIAbUQjLg14uXYEPu4uzaVYSiyBAgtavZRgirFen39elJ7AeaoGFe7Btxba9zqUygVYwt098mprTJWLSxCke8T4F0Qfh+fxco6VsE+njsopB8Q96gkB4lr71gZh5Gg+HwPKq7kA9JUY+S0NLF/ylWABNKUUNcR0+Aj9/Os08uaVRHUIqch4hJjxDJVZRmRKJWMqP/OMUan7LtJMrp7+hx74obbIURNmDpfYkqsuKbmpiagpZ9GJG9reTv9vEfNDltKtQlrjCnVhpH6s+vpfDS8Go/kdzqY//ZuNp4PRtN5p3c+fzt8P5+OB2eX3eRr3McfivoHWt65fIzr9M6PoR2NOkAbjgfD8aCTzScfrn/Pz7Let8BeBMFtAqZSK3E3KqfXGE8rE6MXB57bXWyzk8sTccuaLTokmkS/aApqqYytI87WEWntbQnFWQJr5vAWWfaDYnmEp3LwL0igcaRoIkR/0mts6ms8m02mUlbGGTbKXqFVuylq70qC4jxLIGA0vnxayuVp1Voj0cHleQJsKvQ1fw38zrsWNm3ZPlXxpCHYVOfTuUe2IXr22lsoYDacwP42gYiqND+liJzc/bwkLxXp/AtB5CHUUSO1revPGombbx1qKCDPsqoZO5WPOyjgIntv2qYkL9jwbugd412Tj7LWf5lEszUWVzgirWwznaBYKkvYSvTB2d1H7/n/xuJD7yw41rJbuwHdeCe7z9Y+EUYxIsv2CWy9rSt872v34Fcln5MHKdv+8mAWV0G6DuxvxR/pBtODDiydIjpkpGYAERRgjavvROcQjW+Ss4ropkVrybZNRUfDRisrJmHcGo0DrYXHzZHyYm8xPo7qz/ewQRFz+ADTjFcSZWSIBYkUjjC6M2LGPrkHXC5RS3Hc+KleY1lb6XctTEMpeosnz3OSSo7epsEqh/8pcqWI24n7EvL20aM2U6wC766MSL7/ljP7/f7vAAAA//93ZXtQAQkAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: k3s.cattle.io/v1, Kind=Addon
      objectset.rio.cattle.io/owner-name: metrics-server-deployment
      objectset.rio.cattle.io/owner-namespace: kube-system
    creationTimestamp: "2025-11-28T16:56:24Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      objectset.rio.cattle.io/hash: e10e245e13e46a725c9dddd4f9eb239f147774fd
    name: metrics-server
    namespace: kube-system
    resourceVersion: "2215188"
    uid: dbe7f4ae-7662-4f4f-b234-ecc22370edd3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 0
    selector:
      matchLabels:
        k8s-app: metrics-server
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
        name: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
          image: rancher/mirrored-metrics-server:v0.8.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-28T16:56:26Z"
      lastUpdateTime: "2025-11-28T16:56:26Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-28T16:56:26Z"
      lastUpdateTime: "2025-11-28T16:56:50Z"
      message: ReplicaSet "metrics-server-7bfffcd44" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "10"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"llama-cpp","component":"llm-server"},"name":"llama-cpp","namespace":"llm"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"llama-cpp"}},"template":{"metadata":{"labels":{"app":"llama-cpp","component":"llm-server"}},"spec":{"containers":[{"args":["--model","/models/llama-3.2-3b-instruct-q4_k_m.gguf","--alias","llama-3.2-3b-instruct","--host","0.0.0.0","--port","8080","--ctx-size","4096","--threads","14","--parallel","2","--n-gpu-layers","0","--metrics"],"image":"ghcr.io/ggml-org/llama.cpp:server","imagePullPolicy":"Always","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/health","port":8080},"initialDelaySeconds":60,"periodSeconds":10,"timeoutSeconds":5},"name":"llama-server","ports":[{"containerPort":8080,"name":"http","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/health","port":8080},"initialDelaySeconds":30,"periodSeconds":5,"timeoutSeconds":3},"resources":{"limits":{"cpu":"12000m","memory":"10Gi"},"requests":{"cpu":"6000m","memory":"6Gi"}},"volumeMounts":[{"mountPath":"/models","name":"models","readOnly":true}]}],"nodeSelector":{"hardware":"heavy"},"tolerations":[{"effect":"NoSchedule","key":"heavy","operator":"Equal","value":"true"}],"volumes":[{"name":"models","persistentVolumeClaim":{"claimName":"llama-models-pvc"}}]}}}}
    creationTimestamp: "2025-12-07T16:34:53Z"
    generation: 10
    labels:
      app: llama-cpp
      component: llm-server
    name: llama-cpp
    namespace: llm
    resourceVersion: "831138"
    uid: ac51697c-7dd8-490b-a9ca-767560ace26b
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: llama-cpp
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-07T11:17:40-06:00"
        creationTimestamp: null
        labels:
          app: llama-cpp
          component: llm-server
      spec:
        containers:
        - args:
          - --model
          - /models/llama-3.2-3b-instruct-q4_k_m.gguf
          - --alias
          - llama-3.2-3b-instruct
          - --host
          - 0.0.0.0
          - --port
          - "8080"
          - --ctx-size
          - "4096"
          - --threads
          - "14"
          - --parallel
          - "2"
          - --n-gpu-layers
          - "0"
          - --metrics
          image: ghcr.io/ggml-org/llama.cpp:server
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: llama-server
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "12"
              memory: 10Gi
            requests:
              cpu: "6"
              memory: 6Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /models
            name: models
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          hardware: heavy
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-07T17:01:40Z"
      lastUpdateTime: "2025-12-07T17:18:14Z"
      message: ReplicaSet "llama-cpp-77c6884846" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-12-09T08:53:41Z"
      lastUpdateTime: "2025-12-09T08:53:41Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 10
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"log-analyzer"},"name":"log-analyzer","namespace":"log-analyzer"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"log-analyzer"}},"template":{"metadata":{"labels":{"app":"log-analyzer"}},"spec":{"containers":[{"envFrom":[{"configMapRef":{"name":"log-analyzer-config"}}],"image":"docker.io/library/log-analyzer:latest","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/health","port":8000},"initialDelaySeconds":10,"periodSeconds":10,"timeoutSeconds":5},"name":"log-analyzer","ports":[{"containerPort":8000,"name":"http","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/health","port":8000},"initialDelaySeconds":5,"periodSeconds":5,"timeoutSeconds":3},"resources":{"limits":{"cpu":"1000m","memory":"1Gi"},"requests":{"cpu":"200m","memory":"256Mi"}}}],"nodeSelector":{"hardware":"light"}}}}}
    creationTimestamp: "2025-12-27T01:03:54Z"
    generation: 1
    labels:
      app: log-analyzer
    name: log-analyzer
    namespace: log-analyzer
    resourceVersion: "2216596"
    uid: d855e6a8-12e5-4f8e-ae31-31ea88219c06
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: log-analyzer
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: log-analyzer
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: log-analyzer-config
          image: docker.io/library/log-analyzer:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: log-analyzer
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 200m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          hardware: light
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-27T01:03:54Z"
      lastUpdateTime: "2025-12-27T01:12:21Z"
      message: ReplicaSet "log-analyzer-66c94c6bcf" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-12-27T01:32:45Z"
      lastUpdateTime: "2025-12-27T01:32:45Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: grafana
      meta.helm.sh/release-namespace: logging
    creationTimestamp: "2025-11-29T01:39:41Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: grafana
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.3.0
      helm.sh/chart: grafana-10.2.0
    name: grafana
    namespace: logging
    resourceVersion: "2216514"
    uid: acb37c7b-4bbb-46c6-b95e-a801f37e7a99
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: grafana
        app.kubernetes.io/name: grafana
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 701028326bd9534e4c2a59b8042fd40a2dc8f063a93e89c40b50e72dc8a030d3
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: bed677784356b2af7fb0d87455db21f077853059b594101a4f6532bfbd962a7f
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: grafana
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.3.0
          helm.sh/chart: grafana-10.2.0
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.memory
          - name: TZ
            value: America/Chicago
          image: docker.io/grafana/grafana:12.3.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /etc/grafana/provisioning/datasources/datasources.yaml
            name: config
            subPath: datasources.yaml
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        nodeSelector:
          hardware: light
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: grafana
        serviceAccountName: grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: grafana
          name: config
        - emptyDir: {}
          name: storage
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-29T01:39:41Z"
      lastUpdateTime: "2025-12-27T00:04:11Z"
      message: ReplicaSet "grafana-99648b99f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-12-27T01:32:29Z"
      lastUpdateTime: "2025-12-27T01:32:29Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: logging
    creationTimestamp: "2025-11-29T17:03:12Z"
    generation: 1
    labels:
      app.kubernetes.io/component: single-binary
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: loki
      app.kubernetes.io/part-of: memberlist
      app.kubernetes.io/version: 3.3.0
      helm.sh/chart: loki-6.21.0
    name: loki
    namespace: logging
    resourceVersion: "831130"
    uid: b1ed6ca6-b0e2-4ab9-84a2-4f0725e76082
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Delete
      whenScaled: Delete
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: single-binary
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: loki
    serviceName: loki-headless
    template:
      metadata:
        annotations:
          checksum/config: acd91195e4aa0be5d850ac776fa4b0858625d602c36f10b22a26ee672ff9376c
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: single-binary
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: loki
          app.kubernetes.io/part-of: memberlist
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: single-binary
              topologyKey: kubernetes.io/hostname
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: loki_rule
          - name: FOLDER
            value: /rules
          - name: RESOURCE
            value: both
          - name: WATCH_SERVER_TIMEOUT
            value: "60"
          - name: WATCH_CLIENT_TIMEOUT
            value: "60"
          - name: LOG_LEVEL
            value: INFO
          image: kiwigrid/k8s-sidecar:1.28.0
          imagePullPolicy: IfNotPresent
          name: loki-sc-rules
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rules
            name: sc-rules-volume
        - args:
          - -config.file=/etc/loki/config/config.yaml
          - -target=all
          image: docker.io/grafana/loki:3.3.0
          imagePullPolicy: IfNotPresent
          name: loki
          ports:
          - containerPort: 3100
            name: http-metrics
            protocol: TCP
          - containerPort: 9095
            name: grpc
            protocol: TCP
          - containerPort: 7946
            name: http-memberlist
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /etc/loki/config
            name: config
          - mountPath: /etc/loki/runtime-config
            name: runtime-config
          - mountPath: /var/loki
            name: storage
          - mountPath: /rules
            name: sc-rules-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        nodeSelector:
          hardware: heavy
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - emptyDir: {}
          name: tmp
        - configMap:
            defaultMode: 420
            items:
            - key: config.yaml
              path: config.yaml
            name: loki
          name: config
        - configMap:
            defaultMode: 420
            name: loki-runtime
          name: runtime-config
        - emptyDir: {}
          name: sc-rules-volume
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: storage
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 200Gi
        selector:
          matchLabels:
            app: loki
            type: local
        storageClassName: local-storage
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: loki-c77b86bc4
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: loki-c77b86bc4
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: tempo
      meta.helm.sh/release-namespace: logging
    creationTimestamp: "2025-12-26T23:58:24Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: tempo
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: tempo
      app.kubernetes.io/version: 2.9.0
      helm.sh/chart: tempo-1.24.1
    name: tempo
    namespace: logging
    resourceVersion: "2210447"
    uid: 891824db-4a81-4078-a1bf-9f6119ad0e64
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: tempo
        app.kubernetes.io/name: tempo
    serviceName: tempo-headless
    template:
      metadata:
        annotations:
          checksum/config: 996cf0f8e22844c282d32247b2b10b744a9d2f610d454d7c84524d225a35356a
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: tempo
          app.kubernetes.io/name: tempo
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - -config.file=/conf/tempo.yaml
          - -mem-ballast-size-mbs=1024
          image: docker.io/grafana/tempo:2.9.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 3200
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: tempo
          ports:
          - containerPort: 3200
            name: prom-metrics
            protocol: TCP
          - containerPort: 6831
            name: jaeger-thrift-c
            protocol: UDP
          - containerPort: 6832
            name: jaeger-thrift-b
            protocol: UDP
          - containerPort: 14268
            name: jaeger-thrift-h
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55680
            name: otlp-legacy
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 55681
            name: otlp-httplegacy
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 55678
            name: opencensus
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 3200
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: tempo-conf
          - mountPath: /var/tempo
            name: storage
        dnsPolicy: ClusterFirst
        nodeSelector:
          hardware: heavy
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: tempo
        serviceAccountName: tempo
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - configMap:
            defaultMode: 420
            name: tempo
          name: tempo-conf
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: storage
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 50Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: tempo-7689f4844b
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updateRevision: tempo-7689f4844b
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "2"
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/8xVUW/jNgz+KwOfbde5NG1iYA9F2w3FdmmQ5PZyKApaphstsiRItFuj8H8f5KSpi8ut3XYPQ4AkksiP1MeP1DOglX+Q89JoyACt9SfNCCLYSl1ABldIldErYoigIsYCGSF7BtTaMLI02oelyf8kwZ44cdIkApkVJdKcyIAB0XfPzaMmFz80W8jgpBlFP/0mdfHzilwjBb3rp7EiyIB0Y9p49/2ATI/Yxr71TFW8MRUpzA/baVpOJuX57EPQ3qJ4xX+LDF0EwlHPwFpW5BkrC5mulYpAYU7qb3nZoN9ABoVI0/GsGM9G01l+NpueEeazyacZpZ8m5+NpSdPZaILjUA/fCGE0O6MUuWQ79gM0bQrypEiwcZBBicrTOy6+Ef+JvQ9gv0ffPr5vhMrjf5pFLM7PJjQ6LWEH9BJsW+c0iOEtiVCHV3aeoUIWm98PJUJrf0AWXRcBU2UVMvVBBr3yAZ38mCT+5yUf1ANrNpWpNe9b/UKIsFqbLWnIegFHEKKh1OQ8ZF+fA2j/u7/Eanl5v7hdriGCBlUdtqYpdNEbg+XF/Nfr1cAkTfrPyRvLq+vV+n6xvF3fDizXl4tvbd7GG4/S07NvjG4Wh4C/OFOF65aSVLGk8vB/gRz63zNy7ZON8Xyz8NB13V0EssKHAOZQiw25k62S1pKLVZ41aXKajMawN1rUSi2MkqKFDG7KueGFI0+a4dBdKo9Z2HiaQgTWON5ReWB2YRxDNk0jCDm8ro55O8NGGPVCzV0EjrypnaAg4VBcErWT3F4azfTEvfTRYi6VZEk7nRcFZF9hfr2+v7j6fDOHu92drZOmd1Xo/Xw/GHaKDMMtFk6yFKjgaBTfesHKD9WhiRNpm9NE2vvSuEd0xaBwI+ju+oSH4psP5hFEwEaRe3nhgvzKkgRDBnOzEhsqahVm7JYC932OzihKwvxxmph8aIsKPZMLj40NWP10vn6Snn2vmn8DuW/B2CrU9F3kHcblnrWLojDa32rVHne4C61Z2wKZVuyQ6aENtIZOl/rhS3+wm5xPXzQ2KBXmiiAbhaHX2sDa8o1t3+q9tnsR1M6R5nld5eReLlpAlkZQkJeOimNHut/7LL0/sr0kLFrI0q77KwAA//9p9FM3wQgAAA
      objectset.rio.cattle.io/id: ""
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Service
      objectset.rio.cattle.io/owner-name: envoy-envoy-gateway-system-homelab-gateway-00f55f79
      objectset.rio.cattle.io/owner-namespace: envoy-gateway-system
    creationTimestamp: "2025-11-28T18:42:15Z"
    generation: 2
    labels:
      objectset.rio.cattle.io/hash: dc0039d39189b6986eab9529e025738fe8915a31
      svccontroller.k3s.cattle.io/nodeselector: "false"
      svccontroller.k3s.cattle.io/svcname: envoy-envoy-gateway-system-homelab-gateway-00f55f79
      svccontroller.k3s.cattle.io/svcnamespace: envoy-gateway-system
    name: svclb-envoy-envoy-gateway-system-homelab-gateway-00f55-c765e14f
    namespace: kube-system
    resourceVersion: "2026977"
    uid: 69ea67ba-24a6-4a35-9927-c2eb57759e13
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: svclb-envoy-envoy-gateway-system-homelab-gateway-00f55-c765e14f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: svclb-envoy-envoy-gateway-system-homelab-gateway-00f55-c765e14f
          svccontroller.k3s.cattle.io/svcname: envoy-envoy-gateway-system-homelab-gateway-00f55f79
          svccontroller.k3s.cattle.io/svcnamespace: envoy-gateway-system
      spec:
        automountServiceAccountToken: false
        containers:
        - env:
          - name: SRC_PORT
            value: "80"
          - name: SRC_RANGES
            value: 0.0.0.0/0
          - name: DEST_PROTO
            value: TCP
          - name: DEST_PORT
            value: "31046"
          - name: DEST_IPS
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.hostIPs
          image: rancher/klipper-lb:v0.4.13
          imagePullPolicy: IfNotPresent
          name: lb-tcp-80
          ports:
          - containerPort: 80
            hostPort: 80
            name: lb-tcp-80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        hostNetwork: true
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: svclb
        serviceAccountName: svclb
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 2
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "2"
      meta.helm.sh/release-name: alloy
      meta.helm.sh/release-namespace: logging
    creationTimestamp: "2025-11-29T01:30:28Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: alloy
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: alloy
      app.kubernetes.io/part-of: alloy
      app.kubernetes.io/version: v1.11.3
      helm.sh/chart: alloy-1.4.0
    name: alloy
    namespace: logging
    resourceVersion: "2216584"
    uid: a4655d9d-aa4c-401f-b81e-6492e32fadce
  spec:
    minReadySeconds: 10
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: alloy
        app.kubernetes.io/name: alloy
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: alloy
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: alloy
          app.kubernetes.io/name: alloy
      spec:
        containers:
        - args:
          - run
          - /etc/alloy/config.alloy
          - --storage.path=/tmp/alloy
          - --server.http.listen-addr=0.0.0.0:12345
          - --server.http.ui-path-prefix=/
          - --stability.level=generally-available
          env:
          - name: ALLOY_DEPLOY_MODE
            value: helm
          - name: HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: docker.io/grafana/alloy:v1.11.3
          imagePullPolicy: IfNotPresent
          name: alloy
          ports:
          - containerPort: 12345
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 12345
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/alloy
            name: config
        - args:
          - --watched-dir=/etc/alloy
          - --reload-url=http://localhost:12345/-/reload
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.81.0
          imagePullPolicy: IfNotPresent
          name: config-reloader
          resources:
            requests:
              cpu: 10m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/alloy
            name: config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: alloy
        serviceAccountName: alloy
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: heavy
          operator: Equal
          value: "true"
        volumes:
        - configMap:
            defaultMode: 420
            name: alloy
          name: config
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 2
    updatedNumberScheduled: 2
kind: List
metadata:
  resourceVersion: ""
