# Kubernetes Cluster Status Report

**Date:** 2025-12-29
**Migration:** K3S ‚Üí Kubernetes v1.35.0
**Status:** ‚úÖ Infrastructure Complete, ‚ö†Ô∏è Logging Pipeline Issues

---

## Executive Summary

Successfully migrated from K3S to full Kubernetes with FluxCD GitOps. All infrastructure and workloads are deployed and operational. **Critical Issue:** Logs are not appearing in Grafana despite all components running.

---

## Cluster Overview

### Infrastructure Status ‚úÖ

| Component | Status | Version | Location |
|-----------|--------|---------|----------|
| **Control Plane** | ‚úÖ Running | v1.35.0 | node-1 (10.0.0.102) |
| **Worker Node** | ‚úÖ Running | v1.35.0 | node-2 (10.0.0.103) |
| **CNI** | ‚úÖ Running | Flannel (vxlan) | Both nodes |
| **FluxCD** | ‚úÖ Healthy | v2.x | flux-system namespace |
| **Envoy Gateway** | ‚úÖ Running | v1.4.6 | envoy-gateway-system |

### Network Configuration

```yaml
Pod CIDR:     10.42.0.0/16
Service CIDR: 10.43.0.0/16
CNI:          Flannel (vxlan, UDP 8472)
```

### Node Configuration

**node-1 (Control Plane)**
- IP: 10.0.0.102
- Label: `hardware=light`
- Taint: `node-role.kubernetes.io/control-plane:NoSchedule`
- Role: Lightweight workloads (log-analyzer, envoy-gateway)

**node-2 (Worker)**
- IP: 10.0.0.103
- Label: `hardware=heavy`
- Taint: `heavy=true:NoSchedule`
- Role: Heavy workloads (llama-cpp, loki, tempo)
- Storage: Samsung NVMe 476GB at `/mnt/k8s-storage/`

---

## What We Accomplished Today

### 1. FluxCD Infrastructure Fixes ‚úÖ

**Problem 1: Envoy Gateway HelmRepository failing**
- **Error:** `no artifact available for HelmRepository source 'envoyproxy'`
- **Root Cause:** Missing `type: oci` in HelmRepository spec
- **Fix:** Added `type: oci` to `infrastructure/sources/envoyproxy-charts.yaml`
- **Result:** Envoy Gateway HelmRelease now healthy

**Problem 2: Tempo StatefulSet immutability error**
- **Error:** `cannot patch "tempo" with kind StatefulSet: spec: Forbidden`
- **Root Cause:** Attempted to add `storageClassName` to existing volumeClaimTemplates (immutable)
- **Fix:**
  - Removed `existingClaim` from Tempo HelmRelease
  - Switched to StatefulSet auto-created PVCs via `volumeClaimTemplates`
  - Uninstalled and reinstalled Tempo Helm release
- **Result:** Tempo using standard StatefulSet pattern with `storage-tempo-0` PVC

**Problem 3: Missing StorageClass**
- **Error:** PVs and PVCs couldn't bind
- **Fix:** Created `local-storage` StorageClass with `kubernetes.io/no-provisioner`
- **Result:** All PVs bound successfully

### 2. Container Registry Workflow (GHCR) ‚úÖ

**Infrastructure Created:**
```
.env                    # Secure token storage (gitignored)
.env.example            # Template for setup
helpers/build.sh        # Build image with git SHA tags
helpers/push.sh         # Authenticate & push to ghcr.io
helpers/deploy.sh       # Update deployment, commit, reconcile
justfile                # Recipes: build, push, deploy, release
```

**Workflow:**
```bash
just release  # Build ‚Üí Push to ghcr.io ‚Üí Update Git ‚Üí Flux reconciles
```

**Image Published:**
- `ghcr.io/shanks-t/log-analyzer:latest`
- `ghcr.io/shanks-t/log-analyzer:27c1571` (git SHA)
- **Visibility:** Public (changed from private)

### 3. Workloads Deployment ‚úÖ

**Deployed Applications:**

| Workload | Status | Namespace | Node | Image Source |
|----------|--------|-----------|------|--------------|
| **llama-cpp** | ‚úÖ Running | llm | node-2 | ghcr.io/ggml-org/llama.cpp:server |
| **log-analyzer** | ‚úÖ Running | log-analyzer | node-1 | ghcr.io/shanks-t/log-analyzer:latest |

### 4. Envoy Gateway Configuration ‚úÖ

**Problem:** Gateway configuration existed but not deployed
- **Root Cause:** `platform/gateway/` directory not included in infrastructure kustomization
- **Fix:**
  - Moved `platform/gateway/` ‚Üí `infrastructure/gateway/`
  - Added to infrastructure kustomization
  - Created gateway kustomization.yaml

**Gateway Resources Deployed:**
```
‚úÖ GatewayClass:  eg
‚úÖ Gateway:       homelab-gateway (envoy-gateway-system)
‚úÖ HTTPRoute:     grafana-route (logging namespace)
‚úÖ HTTPRoute:     test-route (default namespace)
```

**Access Endpoints:**
- **Grafana:** http://10.0.0.102:31545/grafana/ (NodePort)
- **Grafana:** http://10.0.0.103:31545/grafana/ (NodePort)
- Note: Port 80 requires iptables redirect (not configured yet)

**externalTrafficPolicy Fix:**
- Initially set to `Local` (only works on node where pod runs)
- Patched to `Cluster` (works on all nodes)
- TODO: Update EnvoyProxy manifest (current version may not support field)

---

## Current Cluster State

### FluxCD Kustomizations

```bash
NAME            REVISION           READY   STATUS
flux-system     main@28543ec       True    ‚úÖ Applied
infrastructure  main@28543ec       True    ‚úÖ Applied
workloads       main@21d1c3a       True    ‚úÖ Applied
```

### HelmReleases (5/5 Ready)

```bash
NAMESPACE              NAME      VERSION   READY   STATUS
envoy-gateway-system   eg        v1.4.6    True    ‚úÖ Helm install succeeded
logging                alloy     v1.4.0    True    ‚úÖ Helm install succeeded
logging                grafana   v10.2.0   True    ‚úÖ Helm upgrade succeeded
logging                loki      v6.21.0   True    ‚úÖ Helm install succeeded
logging                tempo     v1.24.1   True    ‚úÖ Helm upgrade succeeded
```

### Storage Status

**PersistentVolumes:**
```
NAME              CAPACITY   STATUS      CLAIM                   PATH
llama-models-pv   20Gi       Available   (ready for use)         /mnt/k8s-storage/models
loki-pv           200Gi      Bound       storage-loki-0          /mnt/k8s-storage/loki
tempo-pv          50Gi       Bound       storage-tempo-0         /mnt/k8s-storage/tempo
```

**PersistentVolumeClaims:**
```
NAMESPACE   NAME              STATUS   VOLUME     CAPACITY
logging     storage-loki-0    Bound    loki-pv    200Gi  ‚úÖ
logging     storage-tempo-0   Bound    tempo-pv   50Gi   ‚úÖ
```

**StorageClass:**
```
NAME            PROVISIONER                  VOLUMEBINDINGMODE
local-storage   kubernetes.io/no-provisioner WaitForFirstConsumer  ‚úÖ
```

### Running Pods

**Infrastructure Pods:**
```
NAMESPACE              POD                                           STATUS
envoy-gateway-system   envoy-gateway-xxx                             Running ‚úÖ
envoy-gateway-system   envoy-homelab-gateway-xxx                     Running ‚úÖ
flux-system            helm-controller-xxx                           Running ‚úÖ
flux-system            kustomize-controller-xxx                      Running ‚úÖ
flux-system            source-controller-xxx                         Running ‚úÖ
kube-system            coredns-xxx (2 replicas)                      Running ‚úÖ
kube-flannel           kube-flannel-ds-xxx (2 nodes)                 Running ‚úÖ
logging                alloy-xxx (DaemonSet, 1/1 nodes)              Running ‚úÖ
logging                grafana-xxx                                   Running ‚úÖ
logging                loki-0 (StatefulSet)                          Running ‚úÖ
logging                tempo-0 (StatefulSet)                         Running ‚úÖ
```

**Workload Pods:**
```
NAMESPACE        POD                    STATUS    NODE     UPTIME
llm              llama-cpp-xxx          Running   node-2   ~1h
log-analyzer     log-analyzer-xxx       Running   node-1   ~30m
```

---

## Critical Issues

### ‚ö†Ô∏è Logs Not Appearing in Grafana

**Symptoms:**
- Grafana UI accessible: http://10.0.0.102:31545/grafana/
- Loki pod running and healthy
- Alloy DaemonSet running (1/1 nodes - only on node-2)
- No logs visible in Grafana Explore

**Potential Root Causes to Investigate:**

1. **Alloy Configuration**
   - Alloy only running on 1 of 2 nodes (DaemonSet showing 1/1)
   - May not be collecting logs from node-1 (control plane)
   - Check: `kubectl logs -n logging alloy-xxx`
   - Check: Alloy ConfigMap for loki endpoint configuration

2. **Loki Data Source in Grafana**
   - Verify Grafana has Loki data source configured
   - Check URL: Should be `http://loki.logging.svc.cluster.local:3100`
   - Test connection in Grafana: Configuration ‚Üí Data Sources ‚Üí Loki ‚Üí Test

3. **Loki Query Issues**
   - Loki may be receiving logs but queries failing
   - Check Loki logs: `kubectl logs -n logging loki-0 -c loki`
   - Verify retention settings (currently 168h = 7 days)

4. **Network Policy / Service Mesh**
   - Verify services are accessible
   - Test: `kubectl run test --image=curlimages/curl --rm -i -- curl http://loki.logging:3100/ready`

5. **Alloy ‚Üí Loki Connection**
   - Alloy may not be shipping logs to Loki
   - Check Alloy's remote_write config
   - Verify no authentication/TLS issues

**Diagnostic Commands:**
```bash
# Check Alloy logs
kubectl logs -n logging daemonset/alloy --tail=50

# Check Loki ingestion
kubectl logs -n logging loki-0 -c loki --tail=50 | grep -i "ingester"

# Verify Loki API
kubectl run test --image=curlimages/curl --rm -i --restart=Never -- \
  curl -s http://loki.logging:3100/loki/api/v1/labels

# Check Grafana data source config
kubectl get configmap -n logging grafana -o yaml | grep -A 20 datasources
```

---

## Migration Plan Progress

| Phase | Status | Details |
|-------|--------|---------|
| 1-7 | ‚úÖ Complete | K8s cluster installed, CNI, nodes labeled/tainted |
| 8 | ‚úÖ Complete | Flux bootstrapped, port 80 working |
| 9.1 | ‚úÖ Complete | PVs created, pointing to existing data |
| 9.2 | ‚úÖ Complete | Infrastructure reconciled via Flux |
| 9.3 | ‚úÖ Complete | Infrastructure monitoring (all HelmReleases healthy) |
| 9.4 | ‚úÖ Complete | Workloads deployed (llama-cpp, log-analyzer) |
| **10** | **‚ö†Ô∏è In Progress** | **Validation & Testing** |

---

## Next Steps (Priority Order)

### 1. üî¥ **CRITICAL: Fix Logging Pipeline**

**Goal:** Get logs flowing into Grafana for observability

**Steps:**
1. Verify Alloy is scraping logs from both nodes
   - Check why DaemonSet shows only 1/1 (should be 2/2 for both nodes)
   - Verify node-1 tolerations in Alloy DaemonSet
2. Verify Loki data source in Grafana
   - Access Grafana UI: http://10.0.0.102:31545/grafana/
   - Configuration ‚Üí Data Sources ‚Üí Check if Loki exists
   - If not, add: URL = `http://loki.logging:3100`
3. Test Loki API directly
   - Verify logs are being ingested
4. Check Alloy ‚Üí Loki connectivity
   - Review Alloy configuration for remote_write endpoint

### 2. ‚öôÔ∏è **Complete Phase 10 Validation**

Once logging works, validate remaining components:

```bash
# Test LLM inference
kubectl run test-llm --image=curlimages/curl --rm -i --restart=Never -- \
  curl http://llama-cpp.llm.svc.cluster.local:8080/health

# Test log analyzer
kubectl run test-logs --image=curlimages/curl --rm -i --restart=Never -- \
  curl http://log-analyzer.log-analyzer.svc.cluster.local:8000/health

# Test trace collection
kubectl run test-tempo --image=curlimages/curl --rm -i --restart=Never -- \
  curl http://tempo.logging:3200/ready
```

### 3. üìù **Documentation & Cleanup**

- [ ] Document Grafana access credentials
- [ ] Create Grafana dashboard for log analysis
- [ ] Clean up old platform/ directory (if exists)
- [ ] Update migration plan with actual completion times
- [ ] Document GHCR workflow for future images

### 4. üöÄ **Optional Enhancements**

- [ ] Set up port 80 ‚Üí 31545 iptables redirect for cleaner URLs
- [ ] Install MetalLB for proper LoadBalancer support
- [ ] Configure Grafana SSO/OIDC (if needed)
- [ ] Set up Prometheus for metrics (currently disabled)
- [ ] Create HttpRoute for log-analyzer service
- [ ] Implement Flux image automation for log-analyzer

---

## GitOps Workflow Summary

**Branches:**
- `main` - Production cluster configuration

**Key Directories:**
```
infrastructure/
‚îú‚îÄ‚îÄ sources/       # HelmRepositories
‚îú‚îÄ‚îÄ storage/       # PVs, StorageClass
‚îú‚îÄ‚îÄ controllers/   # Envoy Gateway
‚îú‚îÄ‚îÄ gateway/       # Gateway, HTTPRoutes
‚îî‚îÄ‚îÄ logging/       # Loki, Grafana, Tempo, Alloy

workloads/
‚îú‚îÄ‚îÄ llm/           # llama-cpp deployment
‚îî‚îÄ‚îÄ log-analyzer/  # FastAPI log analysis service

helpers/
‚îú‚îÄ‚îÄ build.sh       # Build Docker images
‚îú‚îÄ‚îÄ push.sh        # Push to GHCR
‚îî‚îÄ‚îÄ deploy.sh      # Update & deploy via Flux
```

**Deployment Process:**
1. Make code changes locally
2. Run `just release` (builds, pushes to GHCR, updates Git)
3. Flux automatically reconciles changes to cluster
4. Verify with `flux get kustomizations` and `kubectl get pods`

---

## Useful Commands

### FluxCD
```bash
# Check all kustomizations
flux get kustomizations

# Check HelmReleases
flux get helmreleases -A

# Force reconciliation
flux reconcile source git flux-system
flux reconcile kustomization infrastructure
flux reconcile kustomization workloads

# View logs
flux logs --follow --level=info
```

### Cluster Health
```bash
# Check all pods
kubectl get pods -A

# Check specific namespace
kubectl get pods -n logging

# Check storage
kubectl get pv,pvc -A

# Check Gateway
kubectl get gateway,httproute -A
```

### Service Testing
```bash
# Test Grafana (from your Mac)
curl -I http://10.0.0.102:31545/grafana/

# Test Loki API
kubectl run test --image=curlimages/curl --rm -i --restart=Never -- \
  curl http://loki.logging:3100/ready

# Port-forward for local access
kubectl port-forward -n logging svc/grafana 3000:80
```

### Container Registry
```bash
# Build new image
just build

# Push to GHCR
just push

# Full release workflow
just release
```

---

## Known Issues & Workarounds

### Issue 1: EnvoyProxy externalTrafficPolicy field not working
- **Problem:** Setting `externalTrafficPolicy: Cluster` in EnvoyProxy manifest doesn't apply
- **Workaround:** Manually patch service after deployment
  ```bash
  kubectl patch svc -n envoy-gateway-system \
    envoy-envoy-gateway-system-homelab-gateway-00f55f79 \
    -p '{"spec":{"externalTrafficPolicy":"Cluster"}}'
  ```
- **TODO:** Investigate if this field is supported in current Envoy Gateway version

### Issue 2: Alloy DaemonSet only on 1 node
- **Problem:** DaemonSet shows 1/1 instead of 2/2
- **Possible Cause:** Missing toleration for node-1 control-plane taint
- **TODO:** Verify Alloy DaemonSet spec includes control-plane toleration

---

## Success Metrics

‚úÖ **Infrastructure:** 100% (All components deployed and healthy)
‚úÖ **Workloads:** 100% (Both applications running)
‚úÖ **GitOps:** 100% (Flux managing all resources)
‚úÖ **Ingress:** 100% (Grafana accessible via Envoy Gateway)
‚ö†Ô∏è **Observability:** 50% (Metrics working, logs not appearing)

**Overall Migration Status: 90% Complete**

---

## Files Modified in This Session

```
infrastructure/
‚îú‚îÄ‚îÄ sources/envoyproxy-charts.yaml          # Added type: oci
‚îú‚îÄ‚îÄ storage/local-storageclass.yaml         # NEW: Created StorageClass
‚îú‚îÄ‚îÄ storage/kustomization.yaml              # Added StorageClass reference
‚îú‚îÄ‚îÄ logging/tempo-helmrelease.yaml          # Removed existingClaim
‚îú‚îÄ‚îÄ gateway/                                # NEW: Moved from platform/
‚îÇ   ‚îú‚îÄ‚îÄ kustomization.yaml                  # NEW
‚îÇ   ‚îú‚îÄ‚îÄ 01-gatewayclass.yaml
‚îÇ   ‚îú‚îÄ‚îÄ 02-envoy-proxy-config.yaml          # Added toleration, externalTrafficPolicy
‚îÇ   ‚îú‚îÄ‚îÄ 03-gateway.yaml
‚îÇ   ‚îú‚îÄ‚îÄ 04-test-httproute.yaml
‚îÇ   ‚îî‚îÄ‚îÄ 05-grafana-httproute.yaml
‚îî‚îÄ‚îÄ kustomization.yaml                      # Added gateway/ reference

workloads/
‚îî‚îÄ‚îÄ log-analyzer/deployment.yaml            # Updated image to ghcr.io/shanks-t

helpers/
‚îú‚îÄ‚îÄ build.sh                                # NEW: Build workflow
‚îú‚îÄ‚îÄ push.sh                                 # NEW: GHCR push
‚îî‚îÄ‚îÄ deploy.sh                               # NEW: Flux deploy

Root:
‚îú‚îÄ‚îÄ .env.example                            # NEW: Template
‚îú‚îÄ‚îÄ .gitignore                              # Added .env
‚îú‚îÄ‚îÄ justfile                                # Added build/push/deploy/release
‚îî‚îÄ‚îÄ CLUSTER_STATUS.md                       # NEW: This file
```

---

**Last Updated:** 2025-12-29 18:40 CST
**Next Review:** After fixing logging pipeline
