---
# HelmRelease for Grafana Alloy log collection
# Manages: Log collection from all pods (DaemonSet)
# Deployed on: Both nodes (DaemonSet runs on every node)
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: alloy
  namespace: logging
spec:
  interval: 5m  # Check for updates every 5 minutes
  chart:
    spec:
      chart: alloy
      version: '1.4.0'  # Pin to current version
      sourceRef:
        kind: HelmRepository
        name: grafana
        namespace: flux-system
  install:
    createNamespace: false
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
  # Values from inventory/helm-values-alloy.yaml
  values:
    # Controller type: DaemonSet (runs on every node)
    controller:
      type: daemonset
      # Tolerations to run on all nodes
      tolerations:
        # Tolerate node-2 taint (heavy workloads)
        - key: heavy
          operator: Equal
          value: "true"
          effect: NoSchedule
        # Tolerate node-1 taint (control-plane)
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule

    # Resource limits (per pod, one per node)
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi

    # RBAC: Alloy needs permissions to discover pods
    rbac:
      create: true

    # ServiceAccount for pod discovery
    serviceAccount:
      create: true

    # Security context
    securityContext:
      runAsNonRoot: true
      runAsUser: 473
      fsGroup: 473

    # Alloy configuration (River language)
    # This is the core log collection pipeline
    alloy:
      configMap:
        content: |
          // Logging configuration for Alloy itself
          logging {
            level  = "info"
            format = "logfmt"
          }

          // Discover Kubernetes pods on THIS node only
          // This is critical for DaemonSet deployment to avoid duplicate log collection
          discovery.kubernetes "pods" {
            role = "pod"

            // Only discover pods running on the same node as this Alloy instance
            // Uses the HOSTNAME environment variable to filter by node
            selectors {
              role  = "pod"
              field = "spec.nodeName=" + coalesce(env("HOSTNAME"), constants.hostname)
            }
          }

          // Relabel discovered pods to extract useful metadata
          // This transforms Kubernetes metadata into Loki labels
          discovery.relabel "pod_logs" {
            targets = discovery.kubernetes.pods.targets

            // Extract namespace and use as label
            rule {
              source_labels = ["__meta_kubernetes_namespace"]
              action        = "replace"
              target_label  = "namespace"
            }

            // Extract pod name and use as label
            rule {
              source_labels = ["__meta_kubernetes_pod_name"]
              action        = "replace"
              target_label  = "pod"
            }

            // Extract container name and use as label
            rule {
              source_labels = ["__meta_kubernetes_pod_container_name"]
              action        = "replace"
              target_label  = "container"
            }

            // Extract node name and use as label
            rule {
              source_labels = ["__meta_kubernetes_pod_node_name"]
              action        = "replace"
              target_label  = "node"
            }

            // Add job label based on namespace/pod
            rule {
              source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_name"]
              separator     = "/"
              action        = "replace"
              target_label  = "job"
            }
          }

          // Scrape logs from discovered pods
          // This component tails container logs
          loki.source.kubernetes "pod_logs" {
            targets    = discovery.relabel.pod_logs.output
            forward_to = [loki.process.pod_logs.receiver]
          }

          // Process and enrich logs before sending to Loki
          loki.process "pod_logs" {
            // Add static labels to all log lines
            stage.static_labels {
              values = {
                cluster = "homelab",  // Identify your cluster
              }
            }

            // Forward processed logs to Loki
            forward_to = [loki.write.loki_endpoint.receiver]
          }

          // Write logs to Loki
          // This defines the Loki endpoint and authentication
          loki.write "loki_endpoint" {
            endpoint {
              // Loki service address in Kubernetes DNS format
              // This connects to the Loki service we deployed earlier
              url = "http://loki.logging.svc.cluster.local:3100/loki/api/v1/push"

              // No authentication needed for internal cluster communication
              // In production, you might add basic_auth here
            }
          }
